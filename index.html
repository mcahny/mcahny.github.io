<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  <meta name="author" content="Dahun Kim">
  <meta name="description" content="Research Scientist">
  <link rel="alternate" hreflang="en-us" href="/">
  <meta name="theme-color" content="#2962ff">
   
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous"> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
  
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  <link rel="stylesheet" href="/css/academic.css">
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Dahun Kim">
  <link rel="manifest" href="/index.webmanifest">

  <link rel="canonical" href="/">
  <meta property="twitter:card" content="summary">
  <meta property="og:site_name" content="Dahun Kim - Google DeepMind">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Dahun Kim">
  <meta property="og:description" content="Research Scientist"><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "/"
}
</script>
  <title>Dahun Kim</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Dahun Kim</a>
    </div>
    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>

    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Dahun Kim</a>
    </div>

    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content"> 
      <ul class="navbar-nav d-md-inline-flex">
        <li class="nav-item">
          <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#papers" data-target="#papers"><span>Publications</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#awards" data-target="#awards"><span>Awards &amp; Honors</span></a>
        </li>


        <li class="nav-item">
          <a class="nav-link " href="/#activities" data-target="#activities"><span>Activities</span></a>
        </li>


      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>

    </ul>

  </div>
</nav>


<span class="js-widget-page d-none"></span>

  <section id="about" class="home-section wg-about   " style="padding: 30px 0 20px 0;" >
    <div class="container">

<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">
      <img class="avatar avatar-circle" src="/authors/admin/profile.jpg" alt="Avatar">
      <div class="portrait-title">
        <h2>Dahun Kim</h2>
<!--         <h3>Research Scientist</h3> -->
        
        <h3>
          <a href="https://www.deepmind.com/" target="_blank" rel="noopener">
          <span>Google DeepMind</span>
          </a>
        </h3>
      </div>

      
      <td>
      <br>
      <a href="cv/CV_Dahun_Mar_2025.pdf"><b>CV</b></a>
        | <a href="https://scholar.google.com/citations?user=mHpN1xoAAAAJ"><b>Google Scholar</b></a>
        | <a href="https://github.com/mcahny"><b>Github</b></a>
      <br>
      <br>
      </td>
    
    </div>
  </div>
  <div class="col-12 col-lg-8">

    
<br>
<p>I am a  Senior Research Scientist at <strong><span style="color:#4885ed">G</span><span style="color:#db3236">o</span><span style="color:#f4c20d">o</span><span style="color:#4885ed">g</span><span style="color:#3cba54">l</span><span style="color:#db3236">e</span></strong> <a href="https://www.deepmind.com/">DeepMind</a> (MTV, CA).</p>
    
<p>Recently, my research interests are on improving the capabilities of Large Multimodal Models (eg, Gemini), and understanding the interaction of vision and language.</p>

<p>I obtained my Ph.D. and M.S. at <a href="https://kaist.ac.kr/" target="_blank" rel="noopener">KAIST</a>, advised by Professor <a href="http://rcv.kaist.ac.kr">In So Kweon</a>. I have been fortunate to collaborate with <a href="https://research.adobe.com/">Adobe Research</a> (2019), <a href="https://ai.google/research/teams/brain/" target="_blank" rel="noopener">Google Brain</a> (2020), and <a href="https://ai.google/research/" target="_blank" rel="noopener">Google Research</a> (2021). I am a recipient of <a href="http://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia Fellowship</a>, <a href="http://www.qualcomm.com/research/research/university-relations/innovation-fellowship/2021-south-korea">Qualcomm Innovation Fellowship</a> and <a href="https://www.nrf.re.kr/eng/page/4a5d0ace-9cbb-4d21-9b18-92c8464aa23b">Global Ph.D Fellowship from NRF Korea</a>.</p>

    <div class="row">

      <div class="col-md-6">
        <h3>Contact</h3>
        <ul class="ul-contact fa-ul">
          <li>          
            <i class="fa-li fas fa-envelope"></i>
            <div class="description">
              <p class="course">mcahny01 [at] gmail.com</p>
              <p class="course">mcahny [at] google.com</p>
            </div>
          </li>
          <li>  
            <i class="fa-li fas fa-map-marker"></i>
            <div class="description">
              <p class="course">Googleplex, 1600 Amphitheatre Pkwy, Mountain View, CA 94043</p>
            </div> 
          </li>
       
        </ul>        
      </div>

      <div class="col-md-6">
        <h3>Education</h3>
        <ul class="ul-edu fa-ul">
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PhD in EE, KAIST, 2022</p>
              <p class="institution">on "Learning Dense Pixel Features for Video Processing and Understanding"</p>
            </div>
          </li>
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">MS in EE, KAIST, 2018</p>
              <p class="institution">on "Reducing Human Supervision in Supervised Learning"</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">BS in EE, KAIST, 2016</p>
            </div>
          </li>

          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Exchange Student Program, 2014</p>
              <p class="institution">KTH Royal Institute of Technology in Stockholm, Sweden</p>
            </div>
          </li>
        </ul>
      </div>
    </div>
  </div>
</div>

    </div>
  </section>

<section id="experience" class="home-section wg-blank   " style="padding: 10px 0 10px 0;" >
<div class="container">

<div class="row">
  
    <div class="col-lg-12">
    <h1>Academic Activities</h1>
<ul>
  <li><b>Area Chair</b> in NeurIPS 2025, NeurIPS 2024, NeurIPS 2023</li>
  <li><b>Area Chair</b> in ICML 2025</li>
  <li><b>Area Chair</b> in CVPR 2025, CVPR 2024, CVPR 2023</li>
  <li><b>Outstanding Reviewer</b> in CVPR 2021, ECCV 2020</li>
  <li>Reviewer at CVPR, NeurIPS, ICLR, ICCV, ECCV, ICML, AAAI, EG, TPAMI, TNNLS, TIP</li>
</ul>
 
    <h1>Research Experiences</h1>
<ul>

<li> <div style="float:left"><b>Google DeepMind (previously Google Brain)</b>, MTV, CA </div><div style="float:right">Jul 2022 - Present</div> <br>
    Senior Research Scientist, Research Scientist<br>
</li>
  
<li> <div style="float:left"><b>Google Research</b>, LA, CA (virtual) </div><div style="float:right">May 2021 - Jan 2022</div> <br>
    Research Intern, worked with <a href="http://liangchiehchen.com/">Liang-Chieh Chen</a>, and <a href="https://www.linkedin.com/in/jun-xie-2a68515a/">Jun Xie</a>
</li>

<li> <div style="float:left"><b>Google Brain</b>, MTV, CA (virtual) </div><div style="float:right">Jun 2020 - Nov 2020</div> <br>
    Research Intern, worked with <a href="https://weichengkuo.github.io/">Weicheng Kuo</a>, <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, and <a href="https://scholar.google.com/citations?user=nkmDOPgAAAAJ&hl=en">Anelia Anegelova</a> 
</li>

 <li> <div style="float:left"><b>Adobe Research</b>, San Jose, CA</div><div style="float:right">Jun 2019 - Sep 2019</div> <br>
    Research Intern, worked with: <a href="http://joonyoung-cv.github.io">Joon-Young Lee</a>
</li>

<li> <div style="float:left"><b>KAIST</b>, Daejeon, Korea</div> <div style="float:right">Mar 2016 - Feb 2022</div> <br>
    Research Assistant, Robotics and Computer Vision Lab.
</li>

</ul>
    </div>
</div>
    </div>
  </section>

  <section id="papers" class="home-section wg-papers   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      
<div class="row">
  
    <div class="col-lg-12">
      <h1>Publications</h1>
    </div>
    <div class="row">
        <ul class="ul-papers">
        <hr>
          <h2><b>Multimodal AI</b></h2>
        <li>
          <div class="description">
            <p class="authors">Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment</span></p>
           <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Anelia Angelova</span></p>
            <p class="venue"><span style="color:#626567">COLM 2025 </span> </p>
            <p class="resources">
              [
              paper
              ]
            </p>
          </div>
        </li>
        <li>
          <div class="description">
            <p class="authors">Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities</span></p>
            <p class="authors"><span style="color:#626567"><b>Gemini Team, Google</b></span></p>
            <p class="venue"><span style="color:#626567">2025 </span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2507.06261">paper</a> /
              <a href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/">blogpost</a>
              ]
            </p>
          </div>
        </li>
        <li>
          <div class="description">
            <p class="authors">VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models</span></p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, AJ Piergiovanni, Ganesh Mallya, Anelia Angelova</span></p>
            <p class="venue"><span style="color:#626567">CVPR 2025 </span> </p>
            <p class="resources">
              [
              <a href="http://arxiv.org/abs/2504.03970">paper</a> / 
              <a href="https://github.com/google-deepmind/video_comp">data</a>
              ]
            </p>
          </div>
        </li>
        <li>
          <div class="description">
            <p class="authors">Time-Scaling State-Space Models for Dense Video Captioning</span></p>
            <p class="authors"><span style="color:#626567">AJ Piergiovanni, Ganesh Mallya, <b>Dahun Kim</b>, Anelia Angelova</span></p>
            <p class="venue"><span style="color:#626567">BMVC 2025 </span> </p>
            <p class="resources">
              [
              paper
              ]
            </p>
          </div>
        </li>
        <li>
          <div class="description">
            <p class="authors">Whats in a Video: Factorized Autoregressive Decoding for Online Dense Video Captioning</span></p>
            <p class="authors"><span style="color:#626567">AJ Piergiovanni, <b>Dahun Kim</b>, Michael S Ryoo, Isaac Noble, Anelia Angelova</span></p>
            <p class="venue"><span style="color:#626567">Preprint </span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2411.14688">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
          <div class="description">
            <p class="authors">Learning Visual Grounding from Generative Vision and Language Model</span></p>
            <p class="authors"><span style="color:#626567">Shijie Wang, <b>Dahun Kim</b>, Ali Taalimi, Chen Sun, Weicheng Kuo</span></p>
            <p class="venue"><span style="color:#626567">WACV 2025 </span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2407.14563">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
          <div class="description">
            <p class="authors">Region-centric Image-Language Pretraining for Open-Vocabulary Detection</span></p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Anelia Angelova, Weicheng Kuo</span></p>
            <p class="venue"><span style="color:#626567">ECCV 2024 </span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2310.00161">paper</a> /
              <a href="https://github.com/google-research/google-research/tree/master/fvlm/dito">code</a> / 
              <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/dito?hl=en">Google Cloud Vertex AI</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2023_utut.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Textless Unit-to-Unit training for Many-to-Many Multilingual Speech-to-Speech Translation</span></p>
            <p class="authors"><span style="color:#626567">Minsu Kim, Jeongsoo Choi, <b>Dahun Kim</b>, Yong Man Roh</span></p>
            <p class="venue"><span style="color:#626567">TASLP 2024 </span> <span style="font-weight:normal">(IEEE/ACM Transactions on Audio, Speech and Language Processing)</span></span></p>
            <p class="resources"></p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2308.01831">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
          <div class="description">
            <p class="authors">Omnibind: Teach to build unequal-scale modality interaction for omni-bind of all</span></p>
            <p class="authors"><span style="color:#626567">Yuanhuiyi Lyu, Xu Zheng, <b>Dahun Kim</b>, Lin Wang</span></p>
            <p class="venue"><span style="color:#626567">Preprint 2024</span></p>
            <p class="resources"></p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2405.16108">paper</a>
              ]
            </p>
          </div>
        </li>
          
          
          
        <li>
<!--             <div class="img">
              <img src="/papers/images/blank.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities</span></p>
            <p class="authors"><span style="color:#626567">AJ Piergiovanni*, Isaac Nobel*, <b>Dahun Kim</b>, Michael S. Ryoo, Victor Gomes, Anelia Angelova</span></p>
            <p class="venue"><span style="color:#626567">CVPR 2024<br> 
            <span style="font-weight:normal;color:#BB2222"> Featured at Google AI blogpost</span></p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2311.05698">paper</a> / 
              <a href="https://blog.research.google/2023/11/scaling-multimodal-understanding-to.html">Google blogpost</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2023_iccv.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Contrastive Feature Masking Open-Vocabulary Vision Transformer</span></p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Anelia Angelova, Weicheng Kuo</span></p>
            <p class="venue"><span style="color:#626567">ICCV 2023 </span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2309.00775">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2023_cvpr.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers</span></p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Anelia Angelova, Weicheng Kuo</span></p>
            <p class="venue"><span style="color:#626567">CVPR 2023 </span> <span style="font-weight:normal;color:#BB2222"> Highlight presentation - top 2.5% of submissions <br></span>
            <span style="font-weight:normal;color:#BB2222"> Featured at Google AI blogpost</span></p>
            <p class="resources">
              [
              <a href="http://arxiv.org/abs/2305.07011">paper</a> /
              <a href="https://github.com/google-research/google-research/tree/master/fvlm/rovit">code</a> / 
              <a href="https://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html">Google blogpost</a>
              ]
            </p>
          </div>
        </li>
         <li>
<!--           <div class="img">
            <img src="/papers/images/2023_mammut.png" class="img-responsive" alt="">
          </div> -->
          <div class="description">
            <p class="authors">MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks</span></p>
            <p class="authors"><span style="color:#626567">Weicheng Kuo*, AJ Piergiovanni*, <b>Dahun Kim</b><span>&#8224;</span>, Xiyang Luo<span>&#8224;</span>, Ben Caine, Wei Li,  Abhijit Ogale, <br/>
            &nbsp;&nbsp; Luowei Zhou, Andrew Dai, Zhifeng Chen, Claire Cui, Anelia Angelova (*, <span>&#8224;</span> equal contribution)</span></p>
            <p class="venue"><span style="color:#626567">TMLR 2023</span> <br>
            <span style="font-weight:normal;color:#BB2222"> Featured at Google AI blogpost</span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2303.16839">paper</a> / 
                <a href="https://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html">Google blogpost</a>
              ]
            </p>
          </div>
        </li>
          
        <li>
<!--           <div class="img">
            <img src="/papers/images/2023_reclip.png" class="img-responsive" alt="">
          </div> -->
          <div class="description">
            <p class="authors">RECLIP: Resource-Efficient Clip by Training with Small Images</span></p>
      <p class="authors"><span style="color:#626567">Runze Li, <b>Dahun Kim</b>, Bir Bhanu, Weicheng Kuo</span></p>
            <p class="venue"><span style="color:#626567">TMLR 2023</span> <br></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2304.06028">paper</a>
              ]
            </p>
          </div>
        </li>
          
          
          <hr>
          <h2><b>Perception - Object and Video Understanding</b></h2>

        <li>
<!--           <div class="img">
            <img src="/papers/images/2023_mindvps.png" class="img-responsive" alt="">
          </div> -->
          <div class="description">
            <p class="authors">Uni-DVPS: Unified Model for Depth-Aware Video Panoptic Segmentation</span></p>
      <p class="authors"><span style="color:#626567">Ji-Yeon Kim, Hyun-Bin Oh, <b>Dahun Kim</b>, Tae-Hyun Oh</span></p>
             <p class="venue"><span style="color:#626567">RAL-IROS 2024</span> <span style="font-weight:normal; color:#BB2222">Oral presentation</span><br>
            <span style="font-weight:normal; color:#BB2222">Short version at CVPRW 2023 <span style="font-weight:normal;color:#626567">'Vision-Centric Autonomous Driving' Workshop</span><br></p>
            <p class="resources">
              [
                <a href="https://vcad.site/papers/15/CameraReady/paper.pdf">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--           <div class="img">
            <img src="/papers/images/2023_kmax.png" class="img-responsive" alt="">
          </div> -->
          <div class="description">
            <p class="authors">Video-kMaX: A Simple Unified Approach for Online and Near-Online Video Panoptic Segmentation</span></p>
      <p class="authors"><span style="color:#626567">Inkyu Shin, <b>Dahun Kim</b>, Qihang Yu, Jun Xie, Hong-Seok Kim, Bradley Green, In So Kweon, Kuk-Jin Yoon, Liang-Chieh Chen</span></p>
            <p class="venue"><span style="color:#626567">WACV 2024</span>  <span style="font-weight:normal;color:#BB2222">Oral presentation</span><br>
            <span style="font-weight:normal; color:#BB2222">Short version at <a href="https://sites.google.com/view/t4v-cvpr23" style="color:#BB2222; text-decoration: underline">'Transformers for Vision'</a> workshop @ CVPR 2023</span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2304.04694">paper</a> / 
              <a href="https://www.youtube.com/watch?v=gK3bUCNnvGA">video demo</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2022_tip_vps.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Dense Pixel-level Interpretation of Dynamic Scenes with Video Panoptic Segmentation</p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Sanghyun Woo, Joon-Young Lee, In So Kweon</span><br/></p>
            <p class="venue"><span style="color:#626567">TIP 2022 <br></span>
              <span style="font-weight:normal; color:#BB2222">Short version at What is Motion For (WIMF) workshop @ ECCV 2022</span></p>
            <p class="resources">
              [
                <a href="https://ieeexplore.ieee.org/abstract/document/9806364">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2022_cvpr_tubeformer.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">TubeFormer-DeepLab: Video Mask Transformer</p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Jun Xie, Huiyu Wang, Siyuan Qiao, Qihang Yu, Hong-Seok Kim, Hartwig Adam, In So Kweon, Liang-Chieh Chen</span></p>
            <p class="venue"><span style="color:#626567">CVPR 2022 <br></span>
              <span style="font-weight:normal; color:#BB2222">Ranked #1 on <a href="https://competitions.codalab.org/competitions/33634" style="color:#BB2222; text-decoration: underline">SemKITTI-DVPS</a>, &nbsp;#3 on <a href="http://www.cvlibs.net/datasets/kitti//eval_step.php" style="color:#BB2222; text-decoration: underline">KITTI-STEP</a>, and &nbsp;#4 on <a href="https://competitions.codalab.org/competitions/30712" style="color:#BB2222; text-decoration: underline">VSPW 2021</a><br>
              Short version at <a href="https://sites.google.com/view/t4v-cvpr22" style="color:#BB2222; text-decoration: underline">'Transformers for Vision'</a> workshop @ CVPR 2022</span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2205.15361">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2022_cvpr_cmt.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">CMT-DeepLab: Dynamic Clustering Mask Transformers for Panoptic Segmentation</p>
            <p class="authors"><span style="color:#626567">Qihang Yu, Huiyu Wang, <b>Dahun Kim</b>, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen</span></p>
            <p class="venue"><span style="color:#626567">CVPR 2022  <span style="font-weight:normal;color:#BB2222">Oral presentation</span> <span style="font-weight:normal"></span><br>
            </p>
            <p class="resources">
              [
                <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_CMT-DeepLab_Clustering_Mask_Transformers_for_Panoptic_Segmentation_CVPR_2022_paper.pdf">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2021_preprint_oln.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Learning Open-World Object Proposals without Learning to Classify</p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Tsung-Yi Lin, Anelia Angelova, In So Kweon, Weicheng Kuo</span></p>
            <p class="venue"><span style="color:#626567">RAL-ICRA 2022 <br> </span>
            <span style="font-weight:normal; color:#BB2222">Invited paper talk at Open-World Segmentation (UVO) Workshop @ ICCV 2021 <br>
            Received Qualcomm Innovation Award 2021</span><span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2108.06753">paper</a> /
                <a href="https://github.com/mcahny/object_localization_network">code</a> / 
                <a href="https://github.com/tensorflow/models/blob/master/official/legacy/detection/configs/olnmask_config.py">tf2</a> /
                <a href="https://sites.google.com/view/unidentified-video-object/workshop-program">talk</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2022_wacv.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Tailor Me: An Editing Network for Fashion Attribute Shape Manipulation</p>
            <p class="authors"><span style="color:#626567">Youngjoong Kwon, Stefano Petrangeli, <b>Dahun Kim</b>, Haoliang Wang, Vishy Swaminathan, Henry Fuchs</span></p>
            <p class="venue"><span style="color:#626567">WACV 2022 <span style="font-weight:normal"></span></span></p>
            <p class="resources">
              [
                <a href="https://openaccess.thecvf.com/content/WACV2022/html/Kwon_Tailor_Me_An_Editing_Network_for_Fashion_Attribute_Shape_Manipulation_WACV_2022_paper.html">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2021_bmvc.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Global Context and Geometric Priors for Effective Non-Local Self-Attention</p>
            <p class="authors"><span style="color:#626567">Sanghyun Woo, <b>Dahun Kim</b>, Joon-Young Lee, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">BMVC 2021 <br> </span>
              <span style="font-weight:normal; color:#BB2222">Received Bronze Prize, 27th HumanTech Paper Award, Samsung Electronics Co., Ltd</span><span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0894.html">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2021_tech_deeplab2.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">DeepLab2: A TensorFlow Library for Deep Labeling</p>
            <p class="authors"><span style="color:#626567">Mark Weber, Huiyu Wang, Siyuan Qiao, Jun Xie, Maxwell D. Collins, Yukun Zhu, Liangzhe Yuan,<br/>
            &nbsp;&nbsp; <b>Dahun Kim</b>, Qihang Yu, Daniel Cremers, Laura Leal-Taixe, Alan L. Yuille, Florian Schroff, Hartwig Adam, Liang-Chieh Chen</span></p>
            <p class="venue"><span style="color:#626567">Technical report 2021 <span style="font-weight:normal;color:#BB2222">Internal code contribution</span><span style="font-weight:normal"></span></span></p>

            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2106.09748">paper</a> /
                <a href="https://github.com/google-research/deeplab2">code</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2021_cvpr_vps.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Learning to Associate Every Segment for Video Panoptic Segmentation</p>
            <p class="authors"><span style="color:#626567">Sanghyun Woo, <b>Dahun Kim</b>, Joon-Young Lee, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">CVPR 2021 <span style="font-weight:normal"></span></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2106.09453/">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2021_wacv_boundary.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">The Devil is in the Boundary: Exploiting Boundary Representation for Basis-based Instance Segmentation</p>
            <p class="authors"><span style="color:#626567">Myungchul Kim, Sanghyun Woo, <b>Dahun Kim</b>, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">WACV 2021 <span style="font-weight:normal"></span> </span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2011.13241">paper</a>
              ]
            </p>
          </div>
        </li>
                     
        <li>
<!--             <div class="img">
              <img src="/papers/images/2020_bmvc.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Align-and-Attend Network for Globally and Locally Coherent Video Inpainting</p>
            <p class="authors"><span style="color:#626567">Sanghyun Woo, <b>Dahun Kim</b>, KwanYoung Park, Joon-Young Lee, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">BMVC 2020 <span style="font-weight:normal">(Acceptance: 195/670 ≈ 29.1%)</span></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1905.13066">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2020_cvpr_vps.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Video Panoptic Segmentation</p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Sanghyun Woo, Joon-Young Lee, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">CVPR 2020  <span style="font-weight:normal;color:#BB2222">Oral presentation</span> <span style="font-weight:normal">(Acceptance: 335/6656 ≈ 5.0%)</span>
            <br> <span style="font-weight:normal;color:#BB2222">Patented</span></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2006.11339">paper</a> /
                <a href="https://github.com/mcahny/vps">code</a> /
                <a href="https://sites.google.com/view/video-panoptic">project</a>
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2019_pami.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Recurrent Temporal Aggregation Framework for Deep Video Inpainting </p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim*</b>, Sanghyun Woo*, Joon-Young Lee, In So Kweon (* equal contribution) </span></p>
            <p class="venue"><span style="color:#626567">TPAMI 2020 </span> <br>
              <span style="font-weight:normal; color:#BB2222">Received KAIST-Samsung Industry-University Cooperation Best Paper Award</span></p>
            <p class="resources">
              [
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931251">paper</a> /
                <a href="https://github.com/mcahny/Deep-Video-Inpainting"> code </a> 
              ]
            </p>
          </div>
        </li>


        <li>
<!--             <div class="img">
              <img src="/papers/images/2020_aaai_hidetell.jpeg" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Hide-and-Tell: Learning to Bridge Photo Streams for Visual Storytelling</p>
            <p class="authors"><span style="color:#626567">Yunjae Jung, <b>Dahun Kim</b>, Sanghyun Woo, Kyungsu Kim, Sungjin Kim, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">AAAI 2020  <span style="font-weight:normal">(Acceptance: 1591/7737 ≈ 20.6%)</span></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2002.00774">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2019_cvpr_bvdnet.jpg" class="img-responsive" alt="">
            </div> -->
          
          <div class="description">
            <p class="authors">Deep Blind Video Decaptioning by Temporal Aggregation and Recurrence</p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim*</b>, Sanghyun Woo*, Joon-Young Lee, In So Kweon (* equal contribution) </span></p>
            <p class="venue"><span style="color:#626567">CVPR 2019 <span style="font-weight:normal">(Acceptance: 1294/5160 ≈ 25.2%)</span></span></br></p>
            <span style="font-weight:normal; color:#BB2222"> 1st place winner of ECCV 2018 Chalearn LAP Video De-Captioning Challenge</span> 
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1905.02949">paper</a>
                /<a href="https://github.com/mcahny/Deep-Video-Inpainting"> code </a>    
                /<a href="https://www.youtube.com/watch?v=8rcn5RGExO8&t=5s"> video</a>           
                /<a href="https://sites.google.com/view/bvdnet/"> project </a>                
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2019_cvpr_vinet.jpg" class="img-responsive" alt="">
            </div> -->
          
          <div class="description">
            <p class="authors">Deep Video Inpainting</p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim*</b>, Sanghyun Woo*, Joon-Young Lee, In So Kweon (* equal contribution)</span></p>
            <p class="venue"><span style="color:#626567">CVPR 2019 <span style="font-weight:normal">(Acceptance: 1294/5160 ≈ 25.2%)</span></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1905.01639">paper</a>
                /<a href="https://github.com/mcahny/Deep-Video-Inpainting"> code </a>    
                /<a href="https://www.youtube.com/watch?v=RtThGNTvkjY"> video</a>           
                /<a href="https://sites.google.com/view/deepvinet/"> project </a>                
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2019_aaai_cubic.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Self-Supervised Video Representation Learning with Space-Time Cubic Puzzles</p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Donghyeon Cho, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">AAAI 2019 <span style="font-weight:normal;color:#BB2222">Oral presentation</span> <span style="font-weight:normal">(Acceptance: 459/7095 ≈ 6.5%)</span></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1811.09795"> paper </a>
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2019_aaai_csnet.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Discriminative Feature Learning for Unsupervised Video Summarization</p>
            <p class="authors"><span style="color:#626567">Yunjae Jung, Donghyeon Cho, <b>Dahun Kim</b>, Sanghyun Woo, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">AAAI 2019 <span style="font-weight:normal;color:#BB2222">Oral presentation </span><span style="font-weight:normal">(Acceptance: 459/7095 ≈ 6.5%)</span></span><br>
            <span style="font-weight:normal; color:#BB2222">Received Honorable Mention, 25th HumanTech Paper Award, Samsung Electronics Co., Ltd<br>Patented</span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1811.09791"> paper </a>
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2019_mm_retarget.jpg.JPG" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Video Retargeting: Trade-off between Content Preservation and Spatio-temporal Consistency</p>
            <p class="authors"><span style="color:#626567">Donghyeon Cho, Yunjae Jung, Francois Rameau, <b>Dahun Kim</b>, Sanghyun Woo and In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">MM 2019  <span style="font-weight:normal">(Acceptance: 252/936 ≈ 26.9%)</span></span></p>
            <p class="resources">
              [
                <a href="https://dl.acm.org/citation.cfm?id=3350895"> paper </a>
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2019_mm_domain.JPG" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Preserving Semantic and Temporal Consistency for Unpaired Video-to-Video Translation</p>
            <p class="authors"><span style="color:#626567">Kwanyong Park, Sanghyun Woo, <b>Dahun Kim</b>, Donghyeon Cho, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">MM 2019 <span style="font-weight:normal">(Acceptance: 252/936 ≈ 26.9%)</span></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1908.07683"> paper </a>
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2018_nips.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">LinkNet: Relational Embedding for Scene Graph</p>
            <p class="authors"><span style="color:#626567">Sanghyun Woo*, <b>Dahun Kim*</b>, Donghyeon Cho, In So Kweon (* equal contribution)</span></p>
            <p class="venue"><span style="color:#626567">NeurIPS 2018  <span style="font-weight:normal">(Acceptance: 1011/4856 ≈ 20.8%)</span></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1811.06410"> paper </a>
              ]
            </p>
          </div>
        </li>

        <li>
<!--             <div class="img">
              <img src="/papers/images/2018_wacv_teaser.jpg" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Learning Image Representations by Completing Damaged Jigsaw Puzzles</p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Donghyeon Cho, Donggeun Yoo, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">WACV 2018</span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1802.01880"> paper </a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2017_iccv_tpl.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Two Phase Learning for Weakly Supervised Object Localization</p>
            <p class="authors"><span style="color:#626567"><b>Dahun Kim</b>, Donghyeon Cho, Donggeun Yoo, In So Kweon</span></p>
            <p class="venue"><span style="color:#626567">ICCV 2017 <span style="font-weight:normal">(Acceptance: 621/2143 ≈ 28.9%)</span></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/1708.02108"> paper </a>
              ]
            </p>
          </div>
        </li>








                  
          <hr>
          <h2><b>3D Representation - Avatar Modeling</b></h2>


<li>
<!--             <div class="img">
              <img src="/papers/images/2023_iclr.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Neural Image-based Avatars: Generalizable Radiance Fields for Human Avatar Modeling</p>
            <p class="authors"><span style="color:#626567">Youngjoong Kwon, <b>Dahun Kim</b>, Duygu Ceylan, Henry Fuchs</span></p>
            <p class="venue"><span style="color:#626567">ICLR 2023 <br></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2304.04897">paper</a> / 
                <a href="https://youngjoongunc.github.io/nia/">project</a>
              ]
            </p>
          </div>
        </li>
        <li>
<!--             <div class="img">
              <img src="/papers/images/2021_neurips_nhp.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering</p>
            <p class="authors"><span style="color:#626567">Youngjoong Kwon, <b>Dahun Kim</b>, Duygu Ceylan, Henry Fuchs</span></p>
            <p class="venue"><span style="color:#626567">NeurIPS 2021 <span style="font-weight:normal;color:#BB2222">Spotlight presentation</span> <span style="font-weight:normal">(Acceptance: < 3.0%) </span><br>
              <span style="font-weight:normal; color:#BB2222">Received Bronze Prize, 28th HumanTech Paper Award, Samsung Electronics Co., Ltd</span><span style="font-weight:normal"></span></p>              
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2109.07448">paper</a> /
                <a href="https://github.com/YoungJoongUNC/Neural_Human_Performer">code<a> /
                <a href="https://youngjoongunc.github.io/nhp/">project</a>
              ]
            </p>
          </div>
        </li>

                  

                  
        <li>
<!--             <div class="img">
              <img src="/papers/images/2020_mm.png" class="img-responsive" alt="" >
            </div> -->
          <div class="description">
            <p class="authors">Rotationally-Consistent Novel View Synthesis for Humans</p>
            <p class="authors"><span style="color:#626567">Youngjoong Kwon, Stefano Petrangeli, <b>Dahun Kim</b>, Haoliang Wang, Henry Fuchs, Vishy Swaminathan</span></p>
            <p class="venue"><span style="color:#626567">MM 2020 <span style="font-weight:normal">(Acceptance: 472/1698 ≈ 27.8%) </span></span></p>
            <p class="resources">
              [
                <a href="https://dl.acm.org/doi/10.1145/3394171.3413754">paper</a> /
                <a href="https://github.com/YoungJoongUNC/human_video_novel_view_synthesis">dataset</a>
              ]
            </p>
          </div>
        </li>

       <li>
<!--             <div class="img">
              <img src="/papers/images/2020.eccv.nvs.png" class="img-responsive" alt="">
            </div> -->
          <div class="description">
            <p class="authors">Rotationally-Temporally Consistent Novel-View Synthesis of Human Performance Video</p>
            <p class="authors"><span style="color:#626567">Youngjoong Kwon, Stefano Petrangeli, <b>Dahun Kim</b>, Haoliang Wang, Eunbyung Park, Vishy Swaminathan, Henry Fuchs</span></p>
            <p class="venue"><span style="color:#626567">ECCV 2020 <span style="font-weight:normal;color:#BB2222">Spotlight presentation</span> <span style="font-weight:normal">(Acceptance: 265/5025 ≈ 5.3%)</span></span> </p>
            <p class="resources">
              [
                <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490375.pdf">paper</a> /
                <a href="https://github.com/YoungJoongUNC/human_video_novel_view_synthesis">dataset</a> /
                <a href="https://github.com/YoungJoongUNC/human_video_novel_view_synthesis">code</a> 
              ]
            </p>
          </div>
        </li>

    
     
        </ul>      
    </div>  
</div>
    </div>
  </section>



<section id="interns" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Interns whom I had the pleasure to work with</h1>      
<ul>

  <li>Shijiw Wang in Winter 2023, Ph.D. student at Brown University. <br> 
  hosted with Weicheng Kuo</li>

  <li>Runze Li in Summer 2022. Finished Ph.D. at UC Riverside. Now at Google. <br> 
  hosted with Weicheng Kuo</li>

  <li>Inkyu Shin in Summer 2022, Finished Ph.D. at KAIST. Now at TikTok Research. <br> 
  hosted with Liang-Chieh Chen and Jun Xie</li>

  </ul>
    </div>
</div>
    </div>
  </section>


                  

<section id="awards" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Awards &amp; Honors</h1>      
      <!-- <h2 id="ms-students">MS students</h2> -->
<ul>
<li>NSF travel award for Doctoral Consortium, CVPR 2022</li>
<li>Best Ph.D. Thesis Award, EE, KAIST, 2022</li>
<li>Bronze Prize, 28th HumanTech Paper Award, Samsung Electronics Co., Ltd. 2022 ($5,000)</li>
<li>Qualcomm Innovation Award ($4,000), 2021</li>
<li>Outstanding Reviewer Award, IEEE Conference on Computer Vision and Pattern Recognition, 2021</li>
<li>Bronze Prize, 27th HumanTech Paper Award, Samsung Electronics Co., Ltd. 2021 ($5,000)</li>
<li>Outstanding Reviewer Award, European Conference on Computer Vision, 2020</li>
<li>KAIST-Samsung Industry-University Cooperation Best Paper Award ($3,000), 2020</li>
<li>Microsoft Research Asia (MSRA) Ph.D. Fellowship 2019 Winner ($10,000)</li>
<li>Global Ph.D. Fellowship, National Research Foundation of Korea ($60,000 + 3-year full scholarship)</li>
<li>1st Place Award in ChaLearn LAP 2018 Inpainting Challenge Track2 - Video Decaptioning (ECCV 2018 challenge)</li>
<li>Honorable Mention, 25th HumanTech Paper Award, Samsung Electronics Co., Ltd. 2019 ($2,000)</li>
<li>International Computer Vision Summer School (ICVSS) 2018, Sicily, Italy</li>
</ul>
    </div>
</div>
    </div>
  </section>


<section id="awards" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>US Patents</h1>      
      <!-- <h2 id="ms-students">MS students</h2> -->
<ul>
<li>Video Panoptic Segmentation (issued, 11,640,714)</li>
<li>Panoptic Segmentation (issued, 11,256,960)</li>
<li>Electronic device for key frame analysis and control method thereof (issued, 12,175,369)</li>
<li>Methods and apparatus localizing object (s) in vision data (pending, 18,289,725)</li>
<li>Electronic Device and Control Method of Same (pending, 17/554,142)</li>
<li>Method and Device for Hierarchical Learning of Neural Network Based on Weakly Supervised Learning (pending, 16/758,089)</li>
</ul>
    </div>
</div>
    </div>
  </section>

  
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script> 
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    <script>const code_highlighting = false;</script>
    <script>const isSiteThemeDark = false;</script>
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>

    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>

    <script src="/js/academic.min.a8d7005002cb4a052fd6d721e83df9ba.js"></script>


  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
    .
    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>


